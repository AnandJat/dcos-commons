<h1 id="backup">Backup</h1>

<h2 id="backing-up-to-s3">Backing Up to S3</h2>

<p>You can backup an entire cluster’s data and schema to Amazon S3 using the <code class="highlighter-rouge">backup-s3</code> plan. This plan requires the following parameters to run:</p>
<ul>
  <li><code class="highlighter-rouge">SNAPSHOT_NAME</code>: the name of this snapshot. Snapshots for individual nodes will be stored as S3 folders inside of a top level <code class="highlighter-rouge">snapshot</code> folder.</li>
  <li><code class="highlighter-rouge">CASSANDRA_KEYSPACES</code>: the Cassandra keyspaces to backup. The entire keyspace, as well as its schema, will be backed up for each keyspace specified.</li>
  <li><code class="highlighter-rouge">AWS_ACCESS_KEY_ID</code>: the access key ID for the AWS IAM user running this backup.</li>
  <li><code class="highlighter-rouge">AWS_SECRET_ACCESS_KEY</code>: the secret access key for the AWS IAM user running this backup.</li>
  <li><code class="highlighter-rouge">AWS_REGION</code>: the region of the S3 bucket being used to store this backup.</li>
  <li><code class="highlighter-rouge">S3_BUCKET_NAME</code>: the name of the S3 bucket to store this backup in.</li>
</ul>

<p>Make sure that you provision your nodes with enough disk space to perform a backup. Apache Cassandra backups are stored on disk before being uploaded to S3, and will take up as much space as the data currently in the tables, so you’ll need half of your total available space to be free to backup every keyspace at once.</p>

<p>As noted in the documentation for the <a href="#backup-restore-strategy">backup/restore strategy configuration option</a>, it is possible to run transfers to S3 either in serial or in parallel, but care must be taken not to exceed any throughput limits you may have in your cluster. Throughput depends on a variety of factors, including uplink speed, proximity to region where the backups are being uploaded and downloaded, and the performance of the underlying storage infrastructure. You should perform periodic tests in your local environment to understand what you can expect from S3.</p>

<p>You can configure whether snapshots are created and uploaded in serial, the default, or in parallel (see <a href="#backup-restore-strategy">backup/restore strategy</a>), but the serial backup/restore strategy is recommended.</p>

<p>You can initiate this plan from the command line:</p>
<div class="highlighter-rouge"><pre class="highlight"><code>SNAPSHOT_NAME=&lt;my_snapshot&gt;
CASSANDRA_KEYSPACES="space1 space2"
AWS_ACCESS_KEY_ID=&lt;my_access_key_id&gt;
AWS_SECRET_ACCESS_KEY=&lt;my_secret_access_key&gt;
AWS_REGION=us-west-2
S3_BUCKET_NAME=backups
dcos cassandra plan start backup-s3 -p SNAPSHOT_NAME=$SNAPSHOT_NAME -p "CASSANDRA_KEYSPACES=$CASSANDRA_KEYSPACES" -p AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID -p AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY -p AWS_REGION=$AWS_REGION -p S3_BUCKET_NAME=$S3_BUCKET_NAME
</code></pre>
</div>

<p>If you’re backing up multiple keyspaces, they must be separated by spaces and wrapped in quotation marks when supplied to the <code class="highlighter-rouge">plan start</code> command, as in the example above. If the <code class="highlighter-rouge">CASSANDRA_KEYSPACES</code> parameter isn’t supplied, then every keyspace in your cluster will be backed up.</p>

<p><strong>IMPORTANT</strong>: To ensure that sensitive information, such as your AWS secret access key, remains secure, make sure that you’ve set the <code class="highlighter-rouge">core.dcos_url</code> configuration property in the DC/OS CLI to an HTTPS URL.</p>

<h2 id="backing-up-to-azure">Backing up to Azure</h2>

<p>You can also back up to Microsoft Azure using the <code class="highlighter-rouge">backup-azure</code> plan. This plan requires the following parameters to run:</p>

<ul>
  <li><code class="highlighter-rouge">SNAPSHOT_NAME</code>: the name of this snapshot. Snapshots for individual nodes will be stored as gzipped tarballs with the name <code class="highlighter-rouge">node-&lt;POD_INDEX&gt;.tar.gz</code>.</li>
  <li><code class="highlighter-rouge">CASSANDRA_KEYSPACES</code>: the Cassandra keyspaces to backup. The entire keyspace, as well as its schema, will be backed up for each keyspace specified.</li>
  <li><code class="highlighter-rouge">CLIENT_ID</code>: the client ID for the Azure service principal running this backup.</li>
  <li><code class="highlighter-rouge">TENANT_ID</code>: the tenant ID for the tenant that the service principal belongs to.</li>
  <li><code class="highlighter-rouge">CLIENT_SECRET</code>: the service principal’s secret key.</li>
  <li><code class="highlighter-rouge">AZURE_STORAGE_ACCOUNT</code>: the name of the storage account that this backup will be sent to.</li>
  <li><code class="highlighter-rouge">AZURE_STORAGE_KEY</code>: the secret key associated with the storage account.</li>
  <li><code class="highlighter-rouge">CONTAINER_NAME</code>: the name of the container to store this backup in.</li>
</ul>

<p>You can initiate this plan from the command line in the same way as the Amazon S3 backup plan:</p>
<div class="highlighter-rouge"><pre class="highlight"><code>dcos cassandra plan start backup-azure -p SNAPSHOT_NAME=$SNAPSHOT_NAME -p "CASSANDRA_KEYSPACES=$CASSANDRA_KEYSPACES" -p CLIENT_ID=$CLIENT_ID -p TENANT_ID=$TENANT_ID -p CLIENT_SECRET=$CLIENT_SECRET -p AZURE_STORAGE_ACCOUNT=$AZURE_STORAGE_ACCOUNT -p AZURE_STORAGE_KEY=$AZURE_STORAGE_KEY -p CONTAINER_NAME=$CONTAINER_NAME
</code></pre>
</div>

<h1 id="restore">Restore</h1>

<p>All restore plans will restore the schema from every keyspace backed up with the backup plan and populate those keyspaces with the data they contained at the time the snapshot was taken. Downloading and restoration of backups will use the configured backup/restore strategy. This plan assumes that the keyspaces being restored do not already exist in the current cluster, and will fail if any keyspace with the same name is present.</p>

<h2 id="restoring-from-s3">Restoring From S3</h2>

<p>Restoring cluster data is similar to backing it up. The <code class="highlighter-rouge">restore-s3</code> plan assumes that your data is stored in an S3 bucket in the format that <code class="highlighter-rouge">backup-s3</code> uses. The restore plan has the following parameters:</p>
<ul>
  <li><code class="highlighter-rouge">SNAPSHOT_NAME</code>: the snapshot name from the <code class="highlighter-rouge">backup-s3</code> plan.</li>
  <li><code class="highlighter-rouge">AWS_ACCESS_KEY_ID</code>: the access key ID for the AWS IAM user running this restore.</li>
  <li><code class="highlighter-rouge">AWS_SECRET_ACCESS_KEY</code>: the secret access key for the AWS IAM user running this restore.</li>
  <li><code class="highlighter-rouge">AWS_REGION</code>: the region of the S3 bucket being used to store the backup being restored.</li>
  <li><code class="highlighter-rouge">S3_BUCKET_NAME</code>: the name of the S3 bucket where the backup is stored.</li>
</ul>

<p>To initiate this plan from the command line:</p>
<div class="highlighter-rouge"><pre class="highlight"><code>SNAPSHOT_NAME=&lt;my_snapshot&gt;
CASSANDRA_KEYSPACES="space1 space2"
AWS_ACCESS_KEY_ID=&lt;my_access_key_id&gt;
AWS_SECRET_ACCESS_KEY=&lt;my_secret_access_key&gt;
AWS_REGION=us-west-2
S3_BUCKET_NAME=backups
dcos cassandra plan start restore-s3 -p SNAPSHOT_NAME=$SNAPSHOT_NAME -p "CASSANDRA_KEYSPACES=$CASSANDRA_KEYSPACES" -p AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID -p AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY -p AWS_REGION=$AWS_REGION -p S3_BUCKET_NAME=$S3_BUCKET_NAME
</code></pre>
</div>

<h2 id="restoring-from-azure">Restoring From Azure</h2>

<p>You can restore from Microsoft Azure using the <code class="highlighter-rouge">restore-azure</code> plan. This plan requires the following parameters to run:</p>

<ul>
  <li><code class="highlighter-rouge">SNAPSHOT_NAME</code>: the name of this snapshot. Snapshots for individual nodes will be stored as gzipped tarballs with the name <code class="highlighter-rouge">node-&lt;POD_INDEX&gt;.tar.gz</code>.</li>
  <li><code class="highlighter-rouge">CLIENT_ID</code>: the client ID for the Azure service principal running this backup.</li>
  <li><code class="highlighter-rouge">TENANT_ID</code>: the tenant ID for the tenant that the service principal belongs to.</li>
  <li><code class="highlighter-rouge">CLIENT_SECRET</code>: the service principal’s secret key.</li>
  <li><code class="highlighter-rouge">AZURE_STORAGE_ACCOUNT</code>: the name of the storage account that this backup will be sent to.</li>
  <li><code class="highlighter-rouge">AZURE_STORAGE_KEY</code>: the secret key associated with the storage account.</li>
  <li><code class="highlighter-rouge">CONTAINER_NAME</code>: the name of the container to store this backup in.</li>
</ul>

<p>You can initiate this plan from the command line in the same way as the Amazon S3 restore plan:</p>
<div class="highlighter-rouge"><pre class="highlight"><code>dcos cassandra plan start restore-azure -p SNAPSHOT_NAME=$SNAPSHOT_NAME -p CLIENT_ID=$CLIENT_ID -p TENANT_ID=$TENANT_ID -p CLIENT_SECRET=$CLIENT_SECRET -p AZURE_STORAGE_ACCOUNT=$AZURE_STORAGE_ACCOUNT -p AZURE_STORAGE_KEY=$AZURE_STORAGE_KEY -p CONTAINER_NAME=$CONTAINER_NAME
</code></pre>
</div>
