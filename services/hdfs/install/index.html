<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">







<head>
<title>HDFS: Install and Customize</title>
<link rel="stylesheet" type="text/css" media="all" href="/dcos-commons/style/layout.css" />
<link rel="shortcut icon" type="image/png" href="https://mesosphere.com/favicon.ico"/>
<!-- The Dropdown library is written by Stephen Morley, licensed CC0 1.0 Universal (Public Domain Dedication)-->
<link rel="stylesheet" type="text/css" media="all" href="/dcos-commons/style/Dropdown.css" />
<script src="/dcos-commons/style/Dropdown.js"></script>
<style type="text/css">
/* set the background color of menu items */
.dropdown, .dropdown ul { background: #555555; clear: both; }
/* set the background color of active items */
.dropdown li:hover > a, .dropdown li:hover > span, .dropdown li.dropdownOpen > a, .dropdown li.dropdownOpen > span { background: #af87e0; }
/* pad items, set their text color, and fade their background color */
.dropdown a, .dropdown span { padding: 0.25em 0.5em; color: white; transition: background 0.2s; }
/* show '+' on expandable items */
.dropdown span:after { content: " +"; }

/* toc style: remove extra margin between elements */
.section-nav ul { margin: 0; }
</style>
</head>

<body>
<div id="wrapper">

<a href="/dcos-commons">
<img style="float: left; margin-bottom: 2em" src="https://mesosphere.com/wp-content/themes/mesosphere/library/images/assets/dcos-sdk-logo.png" width="250" alt="DC/OS Software Development Kit" />
</a>
<img style="float: right" src="https://img.shields.io/badge/Status-Alpha-BF97F0.svg?style=flat-square" alt="Status: Alpha" />

<ul class="dropdown" style="clear: both">
  <li>
    <a href="/dcos-commons">Home</a>
  </li>
  <li>
    <span>Documentation</span>
    <ul>
      
      
      
      
      <li><a href="/dcos-commons/operations-guide/">SDK Operations Guide</a></li>
      
      
      
      <li><a href="/dcos-commons/developer-guide/">SDK Developer Guide</a></li>
      
      
      
      <li><a href="/dcos-commons/yaml-reference/">YAML Reference</a></li>
      
      
      
      <li><a href="/dcos-commons/faq/">Frequently Asked Questions</a></li>
      
      
      
      <li><a href="/dcos-commons/glossary/">Glossary</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li><a href="/dcos-commons/reference/api">Javadoc Reference</a></li>
      <li><a href="/dcos-commons/reference/swagger-api">REST APIs</a></li>
    </ul>
  </li>
  <li>
    <span>Tutorials</span>
    <ul>
      
      
      
      <li><a href="/dcos-commons/tutorials/kafka-tutorial/">Kafka Tutorial</a></li>
      
      <li><a href="/dcos-commons/tutorials/secrets-tutorial/">Secrets Tutorial</a></li>
      
    </ul>
  </li>
  <li>
    <span>Services</span>
    <ul>
      
      
      
      
      <li>
        
        <span>Cassandra</span>
        
        <ul>
          
          
          
          <li><a href="/dcos-commons/services/cassandra/install/">Install and Customize</a></li>
          
          
          
          <li><a href="/dcos-commons/services/cassandra/service-settings/">Service Settings</a></li>
          
          
          
          <li><a href="/dcos-commons/services/cassandra/security/">Security</a></li>
          
          
          
          <li><a href="/dcos-commons/services/cassandra/cassandra-settings/">Cassandra Settings</a></li>
          
          
          
          <li><a href="/dcos-commons/services/cassandra/node-settings/">Node Settings</a></li>
          
          
          
          <li><a href="/dcos-commons/services/cassandra/uninstall/">Uninstall</a></li>
          
          
          
          <li><a href="/dcos-commons/services/cassandra/quick-start/">Quick Start</a></li>
          
          
          
          <li><a href="/dcos-commons/services/cassandra/connecting-clients/">Connecting Clients</a></li>
          
          
          
          <li><a href="/dcos-commons/services/cassandra/managing/">Managing</a></li>
          
          
          
          <li><a href="/dcos-commons/services/cassandra/common-operations/">Common Operations</a></li>
          
          
          
          <li><a href="/dcos-commons/services/cassandra/overview/">Overview</a></li>
          
          
          
          <li><a href="/dcos-commons/services/cassandra/api-reference/">API Reference</a></li>
          
          
          
          <li><a href="/dcos-commons/services/cassandra/diagnostic-tools/">Diagnostic Tools</a></li>
          
          
          
          <li><a href="/dcos-commons/services/cassandra/disaster-recovery/">Disaster Recovery</a></li>
          
          
          
          <li><a href="/dcos-commons/services/cassandra/troubleshooting/">Troubleshooting</a></li>
          
          
          
          <li><a href="/dcos-commons/services/cassandra/limitations/">Limitations</a></li>
          
          
          
          <li><a href="/dcos-commons/services/cassandra/supported-versions/">Supported Versions</a></li>
          
          
          
          <li><a href="/dcos-commons/services/cassandra/release-notes/">Release Notes</a></li>
          
          
          
          <li><a href="/dcos-commons/services/cassandra/upgrade/">Upgrade</a></li>
          
          
        </ul>
      </li>
      
      <li>
        
        <span>Elastic</span>
        
        <ul>
          
          
          
          <li><a href="/dcos-commons/services/elastic/install/">Install and Customize</a></li>
          
          
          
          <li><a href="/dcos-commons/services/elastic/elastic-x-pack/">X-Pack</a></li>
          
          
          
          <li><a href="/dcos-commons/services/elastic/security/">Security</a></li>
          
          
          
          <li><a href="/dcos-commons/services/elastic/custom-elasticsearch-yaml/">Custom Elasticsearch YAML</a></li>
          
          
          
          <li><a href="/dcos-commons/services/elastic/uninstall/">Uninstall</a></li>
          
          
          
          <li><a href="/dcos-commons/services/elastic/quick-start/">Quick Start</a></li>
          
          
          
          <li><a href="/dcos-commons/services/elastic/connecting-clients/">Connecting Clients</a></li>
          
          
          
          <li><a href="/dcos-commons/services/elastic/managing/">Managing</a></li>
          
          
          
          <li><a href="/dcos-commons/services/elastic/common-operations/">Common Operations</a></li>
          
          
          
          <li><a href="/dcos-commons/services/elastic/diagnostic-tools/">Diagnostic Tools</a></li>
          
          
          
          <li><a href="/dcos-commons/services/elastic/api-reference/">API Reference</a></li>
          
          
          
          <li><a href="/dcos-commons/services/elastic/overview/">Overview</a></li>
          
          
          
          <li><a href="/dcos-commons/services/elastic/disaster-recovery/">Disaster Recovery</a></li>
          
          
          
          <li><a href="/dcos-commons/services/elastic/troubleshooting/">Troubleshooting</a></li>
          
          
          
          <li><a href="/dcos-commons/services/elastic/limitations/">Limitations</a></li>
          
          
          
          <li><a href="/dcos-commons/services/elastic/supported-versions/">Supported Versions</a></li>
          
          
          
          <li><a href="/dcos-commons/services/elastic/release-notes/">Release Notes</a></li>
          
          
          
          <li><a href="/dcos-commons/services/elastic/upgrade/">Upgrade</a></li>
          
          
        </ul>
      </li>
      
      <li>
        
        <span>HDFS</span>
        
        <ul>
          
          
          
          <li><a href="/dcos-commons/services/hdfs/install/">Install and Customize</a></li>
          
          
          
          <li><a href="/dcos-commons/services/hdfs/security/">Security</a></li>
          
          
          
          <li><a href="/dcos-commons/services/hdfs/uninstall/">Uninstall</a></li>
          
          
          
          <li><a href="/dcos-commons/services/hdfs/quick-start/">Quick Start</a></li>
          
          
          
          <li><a href="/dcos-commons/services/hdfs/connecting-clients/">Connecting Clients</a></li>
          
          
          
          <li><a href="/dcos-commons/services/hdfs/managing/">Managing</a></li>
          
          
          
          <li><a href="/dcos-commons/services/hdfs/diagnostic-tools/">Diagnostic Tools</a></li>
          
          
          
          <li><a href="/dcos-commons/services/hdfs/common-operations/">Common Operations</a></li>
          
          
          
          <li><a href="/dcos-commons/services/hdfs/overview/">Overview</a></li>
          
          
          
          <li><a href="/dcos-commons/services/hdfs/api-reference/">API Reference</a></li>
          
          
          
          <li><a href="/dcos-commons/services/hdfs/troubleshooting/">Troubleshooting</a></li>
          
          
          
          <li><a href="/dcos-commons/services/hdfs/limitations/">Limitations</a></li>
          
          
          
          <li><a href="/dcos-commons/services/hdfs/supported-versions/">Supported Versions</a></li>
          
          
          
          <li><a href="/dcos-commons/services/hdfs/release-notes/">Release Notes</a></li>
          
          
          
          <li><a href="/dcos-commons/services/hdfs/upgrade/">Upgrade</a></li>
          
          
        </ul>
      </li>
      
      <li>
        
        <span>Kafka</span>
        
        <ul>
          
          
          
          <li><a href="/dcos-commons/services/kafka/install/">Install and Customize</a></li>
          
          
          
          <li><a href="/dcos-commons/services/kafka/security/">Security</a></li>
          
          
          
          <li><a href="/dcos-commons/services/kafka/uninstall/">Uninstall</a></li>
          
          
          
          <li><a href="/dcos-commons/services/kafka/quick-start/">Quick Start</a></li>
          
          
          
          <li><a href="/dcos-commons/services/kafka/connecting-clients/">Connecting Clients</a></li>
          
          
          
          <li><a href="/dcos-commons/services/kafka/managing/">Managing</a></li>
          
          
          
          <li><a href="/dcos-commons/services/kafka/common-operations/">Common Operations</a></li>
          
          
          
          <li><a href="/dcos-commons/services/kafka/api-reference/">API Reference</a></li>
          
          
          
          <li><a href="/dcos-commons/services/kafka/overview/">Overview</a></li>
          
          
          
          <li><a href="/dcos-commons/services/kafka/diagnostic-tools/">Diagnostic Tools</a></li>
          
          
          
          <li><a href="/dcos-commons/services/kafka/troubleshooting/">Troubleshooting</a></li>
          
          
          
          <li><a href="/dcos-commons/services/kafka/limitations/">Limitations</a></li>
          
          
          
          <li><a href="/dcos-commons/services/kafka/supported-versions/">Supported Versions</a></li>
          
          
          
          <li><a href="/dcos-commons/services/kafka/release-notes/">Release Notes</a></li>
          
          
          
          <li><a href="/dcos-commons/services/kafka/upgrade/">Upgrade</a></li>
          
          
        </ul>
      </li>
      
    </ul>
  </li>
  <li><a href="https://github.com/mesosphere/dcos-commons/blob/master/CONTRIBUTING.md">Contributing</a></li>
  <li><a href="http://chat.dcos.io" target="_blank">Slack</a></li>
</ul>
<h1>HDFS: Install and Customize</h1>
<div id="content">


<p>The default DC/OS Apache HDFS installation provides reasonable defaults for trying out the service, but may not be sufficient for production use. You may require a different configuration depending on the context of the deployment.</p>

<h2 id="prerequisites">Prerequisites</h2>

<ul>
  <li>Depending on your security mode in Enterprise DC/OS, you may <a href="https://docs.mesosphere.com/services/hdfs/hdfs-auth/">need to provision a service account</a> before installing. Only someone with <code class="highlighter-rouge">superuser</code> permission can create the service account.
    <ul>
      <li><code class="highlighter-rouge">strict</code> <a href="https://docs.mesosphere.com/latest/installing/custom/configuration-parameters/#security">security mode</a> requires a service account.</li>
      <li><code class="highlighter-rouge">permissive</code> security mode a service account is optional.</li>
      <li><code class="highlighter-rouge">disabled</code> security mode does not require a service account.</li>
    </ul>
  </li>
  <li>Your cluster must have at least five private nodes.</li>
  <li>Each agent node must have 4 GB of memory and 5 GiB of disk, and each must have these ports available: 8480, 8485, 9000, 9001, 9002, 9005, and 9006, and 9007.</li>
</ul>

<h2 id="default-installation">Default Installation</h2>

<p>To start a basic test cluster with three journal nodes, two name nodes, and three data nodes, run the following command on the DC/OS CLI. Enterprise DC/OS users may need to follow <a href="https://docs.mesosphere.com/services/hdfs/hdfs-auth/">additional instructions</a>, depending on the security mode of the Enterprise DC/OS cluster.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>dcos package <span class="nb">install </span>beta-hdfs
</code></pre></div></div>

<p>This command creates a new instance with the default name <code class="highlighter-rouge">hdfs</code>. Two instances cannot share the same name, so installing additional instances beyond the default instance requires <a href="#custom-installation">customizing the <code class="highlighter-rouge">name</code> at install time</a> for each additional instance.</p>

<p>All <code class="highlighter-rouge">dcos beta-hdfs</code> CLI commands have a <code class="highlighter-rouge">--name</code> argument allowing the user to specify which instance to query. If you do not specify a service name, the CLI assumes a default value matching the package name, i.e. <code class="highlighter-rouge">beta-hdfs</code>. The default value for <code class="highlighter-rouge">--name</code> can be customized via the DC/OS CLI configuration:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>dcos beta-hdfs <span class="nt">--name</span><span class="o">=</span>hdfs &lt;cmd&gt;
</code></pre></div></div>

<p><strong>Note:</strong> Alternatively, you can <a href="https://docs.mesosphere.com/latest/deploying-services/install/">install from the DC/OS web interface</a>. If you install Apache HDFS from the DC/OS web interface, the <code class="highlighter-rouge">dcos beta-hdfs</code> CLI commands are not automatically installed to your workstation. They may be manually installed using the DC/OS CLI:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dcos package <span class="nb">install </span>beta-hdfs <span class="nt">--cli</span>
</code></pre></div></div>

<h2 id="alternate-install-configurations">Alternate install configurations</h2>

<p>The following are some examples of how to customize the installation of your Apache HDFS instance.</p>

<p>In each case, you would create a new Apache HDFS instance using the custom configuration as follows:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>dcos package <span class="nb">install </span>beta-hdfs <span class="nt">--options</span><span class="o">=</span>sample-hdfs.json
</code></pre></div></div>

<p><strong>Recommendation:</strong> Store your custom configuration in source control.</p>

<h3 id="installing-multiple-instances">Installing multiple instances</h3>

<p>By default, the Apache HDFS service is installed with a service name of <code class="highlighter-rouge">hdfs</code>. You may specify a different name using a custom service configuration as follows:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="s2">"service"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"hdfs-other"</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>When the above JSON configuration is passed to the <code class="highlighter-rouge"><span class="k">package</span> <span class="n">install</span> <span class="n">beta</span><span class="p">-</span><span class="n">hdfs</span></code> command via the <code class="highlighter-rouge">--options</code> argument, the new service will use the name specified in that JSON configuration:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>dcos package <span class="nb">install </span>beta-hdfs <span class="nt">--options</span><span class="o">=</span>hdfs-other.json
</code></pre></div></div>

<p>Multiple instances of Apache HDFS may be installed into your DC/OS cluster by customizing the name of each instance. For example, you might have one instance of Apache HDFS named <code class="highlighter-rouge">hdfs-staging</code> and another named <code class="highlighter-rouge">hdfs-prod</code>, each with its own custom configuration.</p>

<p>After specifying a custom name for your instance, it can be reached using <code class="highlighter-rouge">dcos beta-hdfs</code> CLI commands or directly over HTTP as described <a href="#addressing-named-instances">below</a>.</p>

<p><strong>Note:</strong> The service name <em>cannot</em> be changed after initial install. Changing the service name would require installing a new instance of the service against the new name, then copying over any data as necessary to the new instance.</p>

<h3 id="installing-into-folders">Installing into folders</h3>

<p>In DC/OS 1.10 and above, services may be installed into <em>folders</em> by specifying a slash-delimited service name. For example:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="s2">"service"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/foldered/path/to/hdfs"</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>The above example will install the service under a path of <code class="highlighter-rouge">foldered</code> =&gt; <code class="highlighter-rouge">path</code> =&gt; <code class="highlighter-rouge">to</code> =&gt; <code class="highlighter-rouge">hdfs</code>. It can then be reached using <code class="highlighter-rouge">dcos beta-hdfs</code> CLI commands or directly over HTTP as described <a href="#addressing-named-instances">below</a>.</p>

<p><strong>Note:</strong> The service folder location <em>cannot</em> be changed after initial install. Changing the service location would require installing a new instance of the service against the new location, then copying over any data as necessary to the new instance.</p>

<h3 id="addressing-named-instances">Addressing named instances</h3>

<p>After you’ve installed the service under a custom name or under a folder, it may be accessed from all <code class="highlighter-rouge">dcos beta-hdfs</code> CLI commands using the <code class="highlighter-rouge">--name</code> argument. By default, the <code class="highlighter-rouge">--name</code> value defaults to the name of the package, or <code class="highlighter-rouge">beta-hdfs</code>.</p>

<p>For example, if you had an instance named <code class="highlighter-rouge">hdfs-dev</code>, the following command would invoke a <code class="highlighter-rouge">pod list</code> command against it:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>dcos beta-hdfs <span class="nt">--name</span><span class="o">=</span>hdfs-dev pod list
</code></pre></div></div>

<p>The same query would be over HTTP as follows:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>curl <span class="nt">-H</span> <span class="s2">"Authorization:token=</span><span class="nv">$auth_token</span><span class="s2">"</span> &lt;dcos_url&gt;/service/hdfs-dev/v1/pod
</code></pre></div></div>

<p>Likewise, if you had an instance in a folder like <code class="highlighter-rouge">/foldered/path/to/hdfs</code>, the following command would invoke a <code class="highlighter-rouge">pod list</code> command against it:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>dcos beta-hdfs <span class="nt">--name</span><span class="o">=</span>/foldered/path/to/hdfs pod list
</code></pre></div></div>

<p>Similarly, it could be queried directly over HTTP as follows:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>curl <span class="nt">-H</span> <span class="s2">"Authorization:token=</span><span class="nv">$auth_token</span><span class="s2">"</span> &lt;dcos_url&gt;/service/foldered/path/to/hdfs-dev/v1/pod
</code></pre></div></div>

<p><strong>Note:</strong> You may add a <code class="highlighter-rouge">-v</code> (verbose) argument to any <code class="highlighter-rouge">dcos beta-hdfs</code> command to see the underlying HTTP queries that are being made. This can be a useful tool to see where the CLI is getting its information. In practice, <code class="highlighter-rouge">dcos beta-hdfs</code> commands are a thin wrapper around an HTTP interface provided by the DC/OS Apache HDFS Service itself.</p>

<h3 id="virtual-networks">Virtual networks</h3>

<p>DC/OS Apache HDFS supports deployment on virtual networks on DC/OS (including the <code class="highlighter-rouge">dcos</code> overlay network), allowing each container (task) to have its own IP address and not use port resources on the agent machines. This can be specified by passing the following configuration during installation:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="s2">"service"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"virtual_network_enabled"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p><strong>Note:</strong> Once the service is deployed on a virtual network, it cannot be updated to use the host network.</p>

<h3 id="example-custom-installation">Example custom installation</h3>

<p>If you are ready to ship into production, you will likely need to customize the deployment to suit the workload requirements of your application(s). Customize the default deployment by creating a JSON file, then pass it to <code class="highlighter-rouge">dcos package install</code> using the <code class="highlighter-rouge">--options</code> parameter.</p>

<p>Sample JSON options file named <code class="highlighter-rouge">sample-hdfs.json</code>:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="s2">"data_node"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"count"</span><span class="p">:</span><span class="w"> </span><span class="mi">10</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>This cluster will have 10 data nodes instead of the default value of 3.
See the Configuration section for a list of fields that can be customized via a options JSON file when the HDFS cluster is created.</p>

<h3 id="minimal-installation">Minimal installation</h3>

<p>Many of the other Infinity services currently support deployment in DC/OS Docker for local testing a local machine. However, DC/OS HDFS currently only supports deployment with an HA name service managed by a Quorum Journal. The resource requirements for such a deployment make it prohibitive to install on a local development machine. The default deployment, is the minimal safe deployment for a DC/OS HDFS cluster. Community contributions to support deployment of a non-HA cluster, e.g. a single name node and data node with no failure detector, would be welcome.</p>

<h2 id="integration-with-dcos-access-controls">Integration with DC/OS access controls</h2>

<p>In Enterprise DC/OS 1.10 and above, you can integrate your SDK-based service with DC/OS ACLs to grant users and groups access to only certain services. You do this by installing your service into a folder, and then restricting access to some number of folders. Folders also allow you to namespace services. For instance, <code class="highlighter-rouge">staging/hdfs</code> and <code class="highlighter-rouge">production/hdfs</code>.</p>

<p>Steps:</p>

<ol>
  <li>In the DC/OS GUI, create a group, then add a user to the group. Or, just create a user. Click <strong>Organization</strong> &gt; <strong>Groups</strong> &gt; <strong>+</strong> or <strong>Organization</strong> &gt; <strong>Users</strong> &gt; <strong>+</strong>. If you create a group, you must also create a user and add them to the group.</li>
  <li>
    <p>Give the user permissions for the folder where you will install your service. In this example, we are creating a user called <code class="highlighter-rouge">developer</code>, who will have access to the <code class="highlighter-rouge">/testing</code> folder.</p>

    <p>Select the group or user you created. Select <strong>ADD PERMISSION</strong> and then toggle to <strong>INSERT PERMISSION STRING</strong>. Add each of the following permissions to your user or group, and then click <strong>ADD PERMISSIONS</strong>.</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  dcos:adminrouter:service:marathon full
  dcos:service:marathon:marathon:services:/testing full
  dcos:adminrouter:ops:mesos full
  dcos:adminrouter:ops:slave full
</code></pre></div>    </div>
  </li>
  <li>Install your service into a folder called <code class="highlighter-rouge">test</code>. Go to <strong>Catalog</strong>, then search for <strong>beta-hdfs</strong>.</li>
  <li>
    <p>Click <strong>CONFIGURE</strong> and change the service name to <code class="highlighter-rouge">/testing/hdfs</code>, then deploy.</p>

    <p>The slashes in your service name are interpreted as folders. You are deploying <code class="highlighter-rouge">hdfs</code> in the <code class="highlighter-rouge">/testing</code> folder. Any user with access to the <code class="highlighter-rouge">/testing</code> folder will have access to the service.</p>
  </li>
</ol>

<p><strong>Important:</strong></p>
<ul>
  <li>Services cannot be renamed. Because the location of the service is specified in the name, you cannot move services between folders.</li>
  <li>DC/OS 1.9 and earlier does not accept slashes in service names. You may be able to create the service, but you will encounter unexpected problems.</li>
</ul>

<h2 id="interacting-with-your-foldered-service">Interacting with your foldered service</h2>

<ul>
  <li>Interact with your foldered service via the DC/OS CLI with this flag: <code class="highlighter-rouge">--name=/path/to/myservice</code>.</li>
  <li>To interact with your foldered service over the web directly, use <code class="highlighter-rouge">http://&lt;dcos-url&gt;/service/path/to/myservice</code>. E.g., <code class="highlighter-rouge">http://&lt;dcos-url&gt;/service/testing/hdfs/v1/endpoints</code>.</li>
</ul>

<h2 id="placement-constraints">Placement Constraints</h2>

<p>Placement constraints allow you to customize where a service is deployed in the DC/OS cluster. Depending on the service, some or all components may be configurable using <a href="http://mesosphere.github.io/marathon/docs/constraints.html">Marathon operators (reference)</a>. For example, <code class="highlighter-rouge">[["hostname", "UNIQUE"]]</code> ensures that at most one pod instance is deployed per agent.</p>

<p>A common task is to specify a list of whitelisted systems to deploy to. To achieve this, use the following syntax for the placement constraint:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[["hostname", "LIKE", "10.0.0.159|10.0.1.202|10.0.3.3"]]
</code></pre></div></div>

<p>You must include spare capacity in this list, so that if one of the whitelisted systems goes down, there is still enough room to repair your service (via <a href="../common-operations#replace-a-pod"><code class="highlighter-rouge">pod replace</code></a>) without requiring that system.</p>

<h4 id="example">Example</h4>
<p>In order to define placement constraints as part of an install or update of a service they should be provided as a JSON encoded string.  For example one can define a placement constraint in an options file as follows:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat </span>options.json
<span class="o">{</span>
  <span class="s2">"hello"</span>: <span class="o">{</span>
    <span class="s2">"placement"</span>: <span class="s2">"[[</span><span class="se">\"</span><span class="s2">hostname</span><span class="se">\"</span><span class="s2">, </span><span class="se">\"</span><span class="s2">UNIQUE</span><span class="se">\"</span><span class="s2">]]"</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>
<p>This file can be referenced to install a <code class="highlighter-rouge">hello-world</code> service.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>dcos package <span class="nb">install </span>hello-world <span class="nt">--options</span><span class="o">=</span>options.json
</code></pre></div></div>
<p>Likewise this file can be referenced to update a <code class="highlighter-rouge">hello-world</code> service.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>dcos hello-world update start <span class="nt">--options</span><span class="o">=</span>options.json
</code></pre></div></div>

<h3 id="regions-and-zones">Regions and Zones</h3>

<p>Placement constraints can be applied to zones by referring to the <code class="highlighter-rouge">@zone</code> key. For example, one could spread pods across a minimum of 3 different zones by specifying the constraint:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[["@zone", "GROUP_BY", "3"]]
</code></pre></div></div>

<p>When the region awareness feature is enabled (currently in beta), the <code class="highlighter-rouge">@region</code> key can also be referenced for defining placement constraints. Any placement constraints that do not reference the <code class="highlighter-rouge">@region</code> key are constrained to the local region.</p>

<h4 id="example-1">Example</h4>

<p>Suppose we have a Mesos cluster with three zones. For balanced placement across those three zones, we would have a configuration like this:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
  ...
  "count": 6,
  "placement": "[[\"@zone\", \"GROUP_BY\", \"3\"]]"
}
</code></pre></div></div>

<p>Instances will all be evenly divided between those three zones.</p>

<h2 id="topology">Topology</h2>

<p>An individual HDFS deployment will colocate name nodes with journal nodes, but it will not colocate two name nodes or two journal nodes on the same agent node in the cluster. Data nodes may be colocated with both name nodes and/or journal nodes. If multiple HDFS clusters are installed, they may share the same agent nodes in the cluster provided that no ports specified in the service configurations conflict for those node types.</p>

<h2 id="initial-deployment-strategy">Initial Deployment Strategy</h2>

<p>HDFS deployment occurs in a series of Phases which are configured in the HDFS service’s Deploy Plan.</p>

<p>The <code class="highlighter-rouge">deploy</code> plan may be viewed either using the DC/OS CLI or directly over HTTP. See the <a href="api-reference.md#view-plan">API Reference</a> for more information on viewing the progress of the <code class="highlighter-rouge">deploy</code> plan.</p>

<h3 id="quorum-journal">Quorum Journal</h3>

<p>The first phase of the installation is the Quorum Journal phase. This phase will deploy three journal nodes to provide a Quorum Journal for the HA name service. Each step in the phase represents an individual journal node.</p>

<h3 id="name-service">Name Service</h3>

<p>The second phase of the installation is deployment of the HA name service. This phase deploys two name nodes.  Needed format and bootstrap operations occur as necessary.</p>

<h3 id="zkfc">ZKFC</h3>

<p>The third phase of the installation is deployment of the ZKFC nodes. This phase deploys two ZKFC nodes to enable ZooKeeper failure detection. Each step represents an individual ZKFC node, and there are always exactly two.</p>

<h3 id="distributed-storage">Distributed Storage</h3>

<p>The fourth and final phase of the installation is deployment of the distributed storage service. This phase deploys the data nodes that are configured to act as storage for the cluster. The number of data nodes can be reconfigured post installation.</p>

<h2 id="configuration-deployment-strategy">Configuration Deployment Strategy</h2>

<p>This configuration update strategy is analogous to the installation procedure above. If the configuration update is accepted, there will be no errors in the generated <code class="highlighter-rouge">deploy</code> plan, and a rolling restart will be performed on all affected nodes to apply the updated configuration.</p>

<h2 id="configuration-options">Configuration Options</h2>

<p>The following describes the most commonly used features of DC/OS Apache HDFS and how to configure them via the DC/OS CLI and the DC/OS GUI. There are two methods of configuring an HDFS cluster. The configuration may be specified using a JSON file during installation via the DC/OS command line (See the Installation section) or via modification to the Service Scheduler’s DC/OS environment at runtime (See the Configuration Update section). Note that some configuration options may only be specified at installation time.</p>

<h3 id="service-configuration">Service Configuration</h3>

<p>The service configuration object contains some properties that MUST be specified during installation and CANNOT be modified after installation is in progress.</p>

<p>For more information on service configuration see <a href="https://docs.mesosphere.com/latest/deploying-services/config-universe-service/">the documentation on configuring a service from the catalog</a>.</p>

<h3 id="node-configuration">Node Configuration</h3>

<p>The node configuration objects correspond to the configuration for nodes in the HDFS cluster. Node configuration MUST be specified during installation and MAY be modified during configuration updates. All of the properties except <code class="highlighter-rouge">disk</code> and <code class="highlighter-rouge">disk_type</code> MAY be modified during the configuration update process.</p>

<h4 id="a-note-on-memory-configuration">A Note on Memory Configuration</h4>

<p>As part of the configuration for each node type, the amount of memory in MB allocated to the node can be specified. This value <em>must</em> be larger than the specified maximum heap size for the given node type. Make sure to allocate enough space for additional memory used by the JVM and other overhead. A good rule of thumb is allocate twice as much memory as the size of the heap (set using either <code class="highlighter-rouge">hdfs.hadoop_heapsize</code> or <code class="highlighter-rouge">&lt;node type&gt;.hadoop_&lt;node type&gt;node_opts</code>).</p>

<h4 id="a-note-on-disk-types">A Note on Disk Types</h4>

<p>As already noted, the disk size and type specifications cannot be modified after initial installation. Furthermore, the following disk volume types are available:</p>

<ul>
  <li><code class="highlighter-rouge">ROOT</code>: Data is stored on the same volume as the agent work directory and the node tasks use the configured amount of disk space.</li>
  <li><code class="highlighter-rouge">MOUNT</code>: Data will be stored on a dedicated, operator-formatted volume attached to the agent. Dedicated MOUNT volumes have performance advantages and a disk error on these MOUNT volumes will be correctly reported to HDFS.</li>
</ul>

<h3 id="hdfs-file-system-configuration">HDFS File System Configuration</h3>

<p>The HDFS file system network configuration, permissions, and compression are configured via the <code class="highlighter-rouge">hdfs</code> JSON object. Once these properties are set at installation time they can not be reconfigured.</p>

<h3 id="operating-system-configuration">Operating System Configuration</h3>

<p>In order for HDFS to function correctly, you must perform several important configuration modifications to the OS hosting the deployment. HDFS requires OS-level configuration settings typical of a production storage server.</p>

<table class="table">

  <tr>
    <th>File</th>
    <th>Setting</th>
    <th>Value</th>
    <th>Reason</th>
  </tr>

   <tr>
    <td>/etc/sysctl.conf</td>
    <td>vm.swappiness</td>
    <td>0</td>
    <td>If the OS swaps out the HDFS processes, they can fail to respond to RPC requests, resulting in the process being marked down by the cluster. This can be particularly troublesome for name nodes and journal nodes.</td>
  </tr>

  <tr>
    <td>/etc/security/limits.conf</td>
    <td>nofile</td>
    <td>unlimited</td>
    <td>If this value is too low, a job that operate on the HDFS cluster may fail due to too may open file handles.</td>
  </tr>

  <tr>
    <td>/etc/security/limits.conf, /etc/security/limits.d/90-nproc.conf</td>
    <td>nproc</td>
    <td>32768</td>
    <td>An HDFS node spawns many threads, which go towards kernel nproc count. If nproc is not set appropriately, the node will be killed.</td>
  </tr>

</table>

</div>
</div>
</body>

</html>
